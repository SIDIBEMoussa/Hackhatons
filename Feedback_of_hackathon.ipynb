{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feedback of hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbApUTYuvTSs8Op9yV/n8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDIBEMoussa/Hackhatons/blob/main/Feedback_of_hackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkHWLjNfSCtz"
      },
      "source": [
        "# <center> <h1>`CE QU'IL FAUT RETENIR DE HACKATHON`</h1>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qheXTEqSYsA1"
      },
      "source": [
        "1.   Il faut mettre le plan en prémière position et énoncer le problématique de l'étude\n",
        "2.   Si le data de description n'est pas fournis explicitement, il faut décrire les variables pour plus de portabilité et accéssibilité à tout le monde"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqP-WHsaYr03"
      },
      "source": [
        "<center> <h1>Data preprocessing </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWNYM2n0TFSA"
      },
      "source": [
        "3.   Après le chargement de dataset, on doit estimer les variables manquantes par colonnes(variables) afin de decider la manière de traitement éfficace a adopter sans perdre beaucoup d'information pour celà on peut s'inspirer du code suivant:\n",
        "\n",
        "* Evidemment s'il existe deux  dataset comme dans les compétition on consulter les deux afin d'économiser le temps s'il savère qu'on a bésoin d'un preprocessing aussi pour **test_set**\n",
        "\n",
        "```\n",
        "# Check if there any missing values in train set\n",
        "\n",
        "ax = train.isna().sum().sort_values().plot(kind = 'barh', figsize = (9, 10))\n",
        "plt.title('Percentage of Missing Values Per Column in Train Set', fontdict={'size':15})\n",
        "for p in ax.patches:\n",
        "    percentage ='{:,.0f}%'.format((p.get_width()/train.shape[0])*100)\n",
        "    width, height =p.get_width(),p.get_height()\n",
        "    x=p.get_x()+width+0.02\n",
        "    y=p.get_y()+height/2\n",
        "    ax.annotate(percentage,(x,y))\n",
        "```\n",
        "\n",
        "4. On vérifier aussi s'il existe des données dupliqués qui peut être souvent source de multicolinéarité\n",
        "\n",
        "\n",
        "```\n",
        "# Check for duplicates\n",
        "\n",
        "data.duplicated().any()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsZHHa6sX1-9"
      },
      "source": [
        "# <h1><center> Exploration </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhQ2CrG97z18"
      },
      "source": [
        "Dans le **seaborn** pour les variables catégorielles on peut remplacer **histplot** par **countplot**.\n",
        "\n",
        "Cette étape est très instéressante nous pouvons facilement régrouper les variables catégorielle \n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "# Category columns\n",
        "cat_cols = ['country','region', 'owns_mobile'] + [x for x in all_data.columns if x.startswith('Q')]\n",
        "num_cols = ['age', 'population']\n",
        "\n",
        "# Change columns to their respective datatypes\n",
        "all_data[cat_cols] = all_data[cat_cols].astype('category')\n",
        "```\n",
        "Ce code permet de competer les modalité prises prises par les variables catégorielles\n",
        "\n",
        "\n",
        "```\n",
        "# Check unique values for each categorical column\n",
        "for col in cat_cols:\n",
        "  print(col, all_data[col].nunique())\n",
        "```\n",
        "Les valeurs manquantes:\n",
        "* soit par mean,min,max\n",
        "* https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "\n",
        "On peut verifier par le code suivant que les NaN a été bien traité\n",
        "\n",
        "\n",
        "```\n",
        "# Confirm that there aren't any missing values\n",
        "all_data[all_data.columns.difference(['target'])].isna().sum().any()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "583MCFJi7zSv"
      },
      "source": [
        "### Feature Engineering\n",
        "#### Try different strategies of dealing with categorical variables\n",
        " - One hot encoding\n",
        " - Label encoding\n",
        " - Target encoding\n",
        " - Reduce the number of unique values..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzI2B3oi7wII"
      },
      "source": [
        "On peut coder facilement les variables catégorielles avec une de du bibliothèque **pandas** en l'occurence **get_dummies**\n",
        "\n",
        "`all_data = pd.get_dummies(data = all_data, columns = cat_cols)`\n",
        "\n",
        "Un script pour enlever les variables inutile de train_set\n",
        "# Select main columns to be used in training\n",
        "\n",
        "main_cols = all_data.columns.difference(['ID', 'target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x18n_ioP7v1f"
      },
      "source": [
        "# A explorer\n",
        "\n",
        "###More Tips\n",
        "- Thorough EDA and domain knowledge sourcing\n",
        "- Re-group Categorical features \n",
        "- More Feature Engineering \n",
        "- Dataset balancing - oversampling, undersampling, SMOTE...\n",
        "- Ensembling of models \n",
        "- Cross-validation: Group folds, Stratified..."
      ]
    }
  ]
}